{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow.keras implementation of one-bit-per-weight CNN for CIFAR 10 \n",
    "##  https://arxiv.org/abs/1907.06916\n",
    "## Mark D. McDonnell, Hesham Mostafa, Runchun Wang, Andre van Schaik,\n",
    "## Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version =  1.13.1\n"
     ]
    }
   ],
   "source": [
    "# select a GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '8'\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.io import savemat,loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import tensorflow\n",
    "print('Tensorflow version = ',tensorflow.__version__)\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, History\n",
    "\n",
    "#from tensorflow.keras import backend as K\n",
    "\n",
    "from ResNetModel import resnet_srelu\n",
    "from Utils import cutout,LR_WarmRestart,GetDataGen,plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#params\n",
    "WhichDataSet = 'CIFAR10'\n",
    "#WhichDataSet = 'CIFAR100'\n",
    "init_lr = 0.1\n",
    "epochs = 300\n",
    "batch_size = 125\n",
    "My_wd=5e-4/2\n",
    "resnet_width = 10\n",
    "resnet_depth = 20\n",
    "UseBinary=True\n",
    "UseCutout=True\n",
    "Loss = 'categorical_crossentropy'\n",
    "Optimizer = SGD(lr=init_lr,decay=0.0, momentum=0.9, nesterov=False)\n",
    "Metrics = ['accuracy']\n",
    "ModelsPath = 'TrainedModels/Tensorflow.keras/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare data\n",
    "if WhichDataSet == 'CIFAR10':\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar10.load_data()\n",
    "else:\n",
    "    (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar100.load_data()\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "K_train = x_train.shape[0]\n",
    "input_shape = x_train.shape[1:]\n",
    "x_train = x_train.astype('float32')/255.0\n",
    "x_test = x_test.astype('float32')#/255.0\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcross_entropy_logits_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        return tensorflow.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d (BinaryConv2D)    (None, 32, 32, 160)  4320        batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 32, 32, 160)  0           binary_conv2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 32, 32, 160)  0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32, 32, 160)  0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_1 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 32, 32, 160)  0           binary_conv2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 32, 32, 160)  0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 32, 32, 160)  0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_2 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 160)  0           binary_conv2d_2[0][0]            \n",
      "                                                                 binary_conv2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 32, 32, 160)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 32, 32, 160)  0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 32, 32, 160)  0           re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_3 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 32, 32, 160)  0           binary_conv2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 32, 32, 160)  0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 32, 32, 160)  0           re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_4 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 160)  0           binary_conv2d_4[0][0]            \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 32, 32, 160)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 32, 160)  0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 32, 32, 160)  0           re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_5 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 32, 32, 160)  0           binary_conv2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32, 32, 160)  0           lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 32, 32, 160)  0           re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_6 (BinaryConv2D)  (None, 32, 32, 160)  230400      lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 160)  0           binary_conv2d_6[0][0]            \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 32, 32, 160)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 32, 32, 160)  0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 32, 32, 160)  0           re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_7 (BinaryConv2D)  (None, 16, 16, 320)  460800      lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 16, 16, 320)  0           binary_conv2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 16, 16, 320)  0           lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 160)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 16, 16, 320)  0           re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 16, 16, 160)  0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_8 (BinaryConv2D)  (None, 16, 16, 320)  921600      lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 320)  0           average_pooling2d[0][0]          \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 320)  0           binary_conv2d_8[0][0]            \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 16, 16, 320)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 16, 16, 320)  0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 16, 16, 320)  0           re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_9 (BinaryConv2D)  (None, 16, 16, 320)  921600      lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 16, 16, 320)  0           binary_conv2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 16, 16, 320)  0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 16, 16, 320)  0           re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_10 (BinaryConv2D) (None, 16, 16, 320)  921600      lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 320)  0           binary_conv2d_10[0][0]           \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 16, 16, 320)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 16, 16, 320)  0           lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 16, 16, 320)  0           re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_11 (BinaryConv2D) (None, 16, 16, 320)  921600      lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 16, 16, 320)  0           binary_conv2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 16, 16, 320)  0           lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 16, 16, 320)  0           re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_12 (BinaryConv2D) (None, 16, 16, 320)  921600      lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 320)  0           binary_conv2d_12[0][0]           \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 16, 16, 320)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 16, 16, 320)  0           lambda_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 16, 16, 320)  0           re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_13 (BinaryConv2D) (None, 8, 8, 640)    1843200     lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 8, 8, 640)    0           binary_conv2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 8, 8, 640)    0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 320)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 8, 8, 640)    0           re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 8, 8, 320)    0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_14 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 640)    0           average_pooling2d_1[0][0]        \n",
      "                                                                 lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 640)    0           binary_conv2d_14[0][0]           \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 8, 8, 640)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 8, 640)    0           lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 8, 8, 640)    0           re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_15 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 8, 8, 640)    0           binary_conv2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 8, 8, 640)    0           lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 8, 8, 640)    0           re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_16 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 640)    0           binary_conv2d_16[0][0]           \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 8, 8, 640)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 8, 8, 640)    0           lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 8, 8, 640)    0           re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_17 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 8, 8, 640)    0           binary_conv2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 8, 8, 640)    0           lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 8, 8, 640)    0           re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_18 (BinaryConv2D) (None, 8, 8, 640)    3686400     lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 640)    0           binary_conv2d_18[0][0]           \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 8, 8, 640)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 8, 8, 640)    0           lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 8, 8, 640)    0           re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "binary_conv2d_19 (BinaryConv2D) (None, 8, 8, 10)     6400        lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 8, 8, 10)     0           binary_conv2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 10)           0           lambda_40[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,737,132\n",
      "Trainable params: 26,737,126\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define a datagen or generating training samples with flip and pad/crop augmentation, and if set to True, with cutout augmentation\n",
    "dataGenerator = GetDataGen(UseCutout)\n",
    "\n",
    "#define and compile the model\n",
    "Temperature=40.0\n",
    "model = resnet_srelu(Temperature,UseBinary,input_shape=input_shape, depth=resnet_depth, num_classes=num_classes,\n",
    "                     wd=My_wd,width=resnet_width)\n",
    "model.compile(loss=catcross_entropy_logits_loss() ,optimizer = Optimizer, metrics = Metrics)\n",
    "\n",
    "#print  the model\n",
    "model.summary()\n",
    "\n",
    "#define the learnng rate schedule\n",
    "steps_per_epoch = int(np.floor(K_train / batch_size))\n",
    "lr_scheduler = LR_WarmRestart(nbatch=steps_per_epoch,\n",
    "                              initial_lr=init_lr, min_lr=init_lr*1e-4,\n",
    "                              epochs_restart = [],\n",
    "                              Tmult=300.0) \n",
    "\n",
    "#define callbacks\n",
    "history = History()\n",
    "callbacks = [lr_scheduler,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mdmcdonn/anaconda3/envs/April2019/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      " Start of Epoch Learning Rate = 0.100000\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 8s 786us/sample - loss: 3.9888 - acc: 0.4307\n",
      "\n",
      " End of Epoch Learning Rate = 0.099997\n",
      "400/400 [==============================] - 121s 303ms/step - loss: 4.8895 - acc: 0.2614 - val_loss: 3.9888 - val_acc: 0.4307\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099997\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 3.1107 - acc: 0.5132\n",
      "\n",
      " End of Epoch Learning Rate = 0.099989\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 3.5624 - acc: 0.4488 - val_loss: 3.1107 - val_acc: 0.5132\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099989\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 2.2573 - acc: 0.6504\n",
      "\n",
      " End of Epoch Learning Rate = 0.099975\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 2.7182 - acc: 0.5582 - val_loss: 2.2573 - val_acc: 0.6504\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099975\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 1.8149 - acc: 0.7171\n",
      "\n",
      " End of Epoch Learning Rate = 0.099956\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 2.1344 - acc: 0.6428 - val_loss: 1.8149 - val_acc: 0.7171\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099956\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 8s 757us/sample - loss: 1.5654 - acc: 0.7455\n",
      "\n",
      " End of Epoch Learning Rate = 0.099931\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 1.7502 - acc: 0.6986 - val_loss: 1.5654 - val_acc: 0.7455\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099931\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 1.4333 - acc: 0.7482\n",
      "\n",
      " End of Epoch Learning Rate = 0.099901\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 1.4871 - acc: 0.7355 - val_loss: 1.4333 - val_acc: 0.7482\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099901\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 1.1844 - acc: 0.7947\n",
      "\n",
      " End of Epoch Learning Rate = 0.099866\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.3186 - acc: 0.7573 - val_loss: 1.1844 - val_acc: 0.7947\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099866\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 1.0634 - acc: 0.8169\n",
      "\n",
      " End of Epoch Learning Rate = 0.099825\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.2045 - acc: 0.7737 - val_loss: 1.0634 - val_acc: 0.8169\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099825\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 1.0329 - acc: 0.8229\n",
      "\n",
      " End of Epoch Learning Rate = 0.099778\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 1.1057 - acc: 0.7911 - val_loss: 1.0329 - val_acc: 0.8229\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099778\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.9763 - acc: 0.8257\n",
      "\n",
      " End of Epoch Learning Rate = 0.099726\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.0569 - acc: 0.7960 - val_loss: 0.9763 - val_acc: 0.8257\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099726\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.9368 - acc: 0.8454\n",
      "\n",
      " End of Epoch Learning Rate = 0.099669\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 1.0098 - acc: 0.8059 - val_loss: 0.9368 - val_acc: 0.8454\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099669\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.8915 - acc: 0.8487\n",
      "\n",
      " End of Epoch Learning Rate = 0.099606\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.9774 - acc: 0.8125 - val_loss: 0.8915 - val_acc: 0.8487\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099606\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 0.8472 - acc: 0.8637\n",
      "\n",
      " End of Epoch Learning Rate = 0.099537\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.9621 - acc: 0.8178 - val_loss: 0.8472 - val_acc: 0.8637\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099537\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.8641 - acc: 0.8516\n",
      "\n",
      " End of Epoch Learning Rate = 0.099464\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.9364 - acc: 0.8254 - val_loss: 0.8641 - val_acc: 0.8516\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099464\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.8820 - acc: 0.8528\n",
      "\n",
      " End of Epoch Learning Rate = 0.099384\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.9315 - acc: 0.8259 - val_loss: 0.8820 - val_acc: 0.8528\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099384\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.8837 - acc: 0.8534\n",
      "\n",
      " End of Epoch Learning Rate = 0.099300\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.9108 - acc: 0.8328 - val_loss: 0.8837 - val_acc: 0.8534\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099300\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.8091 - acc: 0.8740\n",
      "\n",
      " End of Epoch Learning Rate = 0.099210\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.9130 - acc: 0.8310 - val_loss: 0.8091 - val_acc: 0.8740\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099210\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.8601 - acc: 0.8520\n",
      "\n",
      " End of Epoch Learning Rate = 0.099114\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.9056 - acc: 0.8340 - val_loss: 0.8601 - val_acc: 0.8520\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099114\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.8564 - acc: 0.8561\n",
      "\n",
      " End of Epoch Learning Rate = 0.099014\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8994 - acc: 0.8402 - val_loss: 0.8564 - val_acc: 0.8561\n",
      "\n",
      " Start of Epoch Learning Rate = 0.099014\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 0.8142 - acc: 0.8734\n",
      "\n",
      " End of Epoch Learning Rate = 0.098907\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8848 - acc: 0.8427 - val_loss: 0.8142 - val_acc: 0.8734\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098907\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.8718 - acc: 0.8517\n",
      "\n",
      " End of Epoch Learning Rate = 0.098796\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8821 - acc: 0.8441 - val_loss: 0.8718 - val_acc: 0.8517\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098796\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 0.7960 - acc: 0.8788\n",
      "\n",
      " End of Epoch Learning Rate = 0.098679\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8771 - acc: 0.8449 - val_loss: 0.7960 - val_acc: 0.8788\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098679\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 0.8198 - acc: 0.8736\n",
      "\n",
      " End of Epoch Learning Rate = 0.098557\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8750 - acc: 0.8456 - val_loss: 0.8198 - val_acc: 0.8736\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098557\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.8475 - acc: 0.8627\n",
      "\n",
      " End of Epoch Learning Rate = 0.098429\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8747 - acc: 0.8461 - val_loss: 0.8475 - val_acc: 0.8627\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098429\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.8043 - acc: 0.8731\n",
      "\n",
      " End of Epoch Learning Rate = 0.098296\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.8549 - acc: 0.8534 - val_loss: 0.8043 - val_acc: 0.8731\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098296\n",
      "Epoch 26/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.8085 - acc: 0.8752\n",
      "\n",
      " End of Epoch Learning Rate = 0.098158\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8586 - acc: 0.8514 - val_loss: 0.8085 - val_acc: 0.8752\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098158\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.8123 - acc: 0.8726\n",
      "\n",
      " End of Epoch Learning Rate = 0.098015\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.8756 - acc: 0.8468 - val_loss: 0.8123 - val_acc: 0.8726\n",
      "\n",
      " Start of Epoch Learning Rate = 0.098015\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 0.8017 - acc: 0.8765\n",
      "\n",
      " End of Epoch Learning Rate = 0.097866\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8771 - acc: 0.8492 - val_loss: 0.8017 - val_acc: 0.8765\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097866\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.8357 - acc: 0.8628\n",
      "\n",
      " End of Epoch Learning Rate = 0.097712\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8523 - acc: 0.8568 - val_loss: 0.8357 - val_acc: 0.8628\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097712\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.8433 - acc: 0.8669\n",
      "\n",
      " End of Epoch Learning Rate = 0.097553\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8614 - acc: 0.8526 - val_loss: 0.8433 - val_acc: 0.8669\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097553\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 0.8179 - acc: 0.8726\n",
      "\n",
      " End of Epoch Learning Rate = 0.097389\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8609 - acc: 0.8534 - val_loss: 0.8179 - val_acc: 0.8726\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097389\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 0.7691 - acc: 0.8874\n",
      "\n",
      " End of Epoch Learning Rate = 0.097219\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8552 - acc: 0.8571 - val_loss: 0.7691 - val_acc: 0.8874\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097219\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.8089 - acc: 0.8770\n",
      "\n",
      " End of Epoch Learning Rate = 0.097044\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8465 - acc: 0.8589 - val_loss: 0.8089 - val_acc: 0.8770\n",
      "\n",
      " Start of Epoch Learning Rate = 0.097044\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.7762 - acc: 0.8881\n",
      "\n",
      " End of Epoch Learning Rate = 0.096864\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8499 - acc: 0.8564 - val_loss: 0.7762 - val_acc: 0.8881\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096864\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.8001 - acc: 0.8832\n",
      "\n",
      " End of Epoch Learning Rate = 0.096679\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8455 - acc: 0.8597 - val_loss: 0.8001 - val_acc: 0.8832\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096679\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.7976 - acc: 0.8780\n",
      "\n",
      " End of Epoch Learning Rate = 0.096489\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8536 - acc: 0.8572 - val_loss: 0.7976 - val_acc: 0.8780\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096489\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 0.7915 - acc: 0.8832\n",
      "\n",
      " End of Epoch Learning Rate = 0.096294\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8383 - acc: 0.8630 - val_loss: 0.7915 - val_acc: 0.8832\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096294\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 0.8570 - acc: 0.8609\n",
      "\n",
      " End of Epoch Learning Rate = 0.096094\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8367 - acc: 0.8609 - val_loss: 0.8570 - val_acc: 0.8609\n",
      "\n",
      " Start of Epoch Learning Rate = 0.096094\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 0.7856 - acc: 0.8806\n",
      "\n",
      " End of Epoch Learning Rate = 0.095888\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.8292 - acc: 0.8642 - val_loss: 0.7856 - val_acc: 0.8806\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095888\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.7982 - acc: 0.8769\n",
      "\n",
      " End of Epoch Learning Rate = 0.095678\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8324 - acc: 0.8604 - val_loss: 0.7982 - val_acc: 0.8769\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095678\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.7981 - acc: 0.8808\n",
      "\n",
      " End of Epoch Learning Rate = 0.095462\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8265 - acc: 0.8624 - val_loss: 0.7981 - val_acc: 0.8808\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095462\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.7553 - acc: 0.8923\n",
      "\n",
      " End of Epoch Learning Rate = 0.095242\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8262 - acc: 0.8647 - val_loss: 0.7553 - val_acc: 0.8923\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095242\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.7980 - acc: 0.8761\n",
      "\n",
      " End of Epoch Learning Rate = 0.095016\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8270 - acc: 0.8637 - val_loss: 0.7980 - val_acc: 0.8761\n",
      "\n",
      " Start of Epoch Learning Rate = 0.095016\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.7835 - acc: 0.8827\n",
      "\n",
      " End of Epoch Learning Rate = 0.094786\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8170 - acc: 0.8662 - val_loss: 0.7835 - val_acc: 0.8827\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094786\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.8063 - acc: 0.8756\n",
      "\n",
      " End of Epoch Learning Rate = 0.094551\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8200 - acc: 0.8660 - val_loss: 0.8063 - val_acc: 0.8756\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094551\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.7315 - acc: 0.8973\n",
      "\n",
      " End of Epoch Learning Rate = 0.094311\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8200 - acc: 0.8661 - val_loss: 0.7315 - val_acc: 0.8973\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094311\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 0.7404 - acc: 0.8979\n",
      "\n",
      " End of Epoch Learning Rate = 0.094066\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8259 - acc: 0.8640 - val_loss: 0.7404 - val_acc: 0.8979\n",
      "\n",
      " Start of Epoch Learning Rate = 0.094066\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.7764 - acc: 0.8860\n",
      "\n",
      " End of Epoch Learning Rate = 0.093816\n",
      "400/400 [==============================] - 118s 295ms/step - loss: 0.8177 - acc: 0.8642 - val_loss: 0.7764 - val_acc: 0.8860\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093816\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.7541 - acc: 0.8962\n",
      "\n",
      " End of Epoch Learning Rate = 0.093561\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8167 - acc: 0.8666 - val_loss: 0.7541 - val_acc: 0.8962\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093561\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 8s 755us/sample - loss: 0.7720 - acc: 0.8867\n",
      "\n",
      " End of Epoch Learning Rate = 0.093302\n",
      "400/400 [==============================] - 118s 295ms/step - loss: 0.8116 - acc: 0.8677 - val_loss: 0.7720 - val_acc: 0.8867\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093302\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.7792 - acc: 0.8847\n",
      "\n",
      " End of Epoch Learning Rate = 0.093038\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.8059 - acc: 0.8694 - val_loss: 0.7792 - val_acc: 0.8847\n",
      "\n",
      " Start of Epoch Learning Rate = 0.093038\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 758us/sample - loss: 0.7809 - acc: 0.8836\n",
      "\n",
      " End of Epoch Learning Rate = 0.092769\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.8123 - acc: 0.8670 - val_loss: 0.7809 - val_acc: 0.8836\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092769\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.8365 - acc: 0.8636\n",
      "\n",
      " End of Epoch Learning Rate = 0.092495\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8137 - acc: 0.8660 - val_loss: 0.8365 - val_acc: 0.8636\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092495\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.7736 - acc: 0.8873\n",
      "\n",
      " End of Epoch Learning Rate = 0.092217\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.8128 - acc: 0.8682 - val_loss: 0.7736 - val_acc: 0.8873\n",
      "\n",
      " Start of Epoch Learning Rate = 0.092217\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.8057 - acc: 0.8820\n",
      "\n",
      " End of Epoch Learning Rate = 0.091934\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.8030 - acc: 0.8699 - val_loss: 0.8057 - val_acc: 0.8820\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091934\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.7979 - acc: 0.8749\n",
      "\n",
      " End of Epoch Learning Rate = 0.091647\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7973 - acc: 0.8691 - val_loss: 0.7979 - val_acc: 0.8749\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091647\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.7670 - acc: 0.8819\n",
      "\n",
      " End of Epoch Learning Rate = 0.091355\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7877 - acc: 0.8727 - val_loss: 0.7670 - val_acc: 0.8819\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091355\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.7285 - acc: 0.8966\n",
      "\n",
      " End of Epoch Learning Rate = 0.091058\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7890 - acc: 0.8714 - val_loss: 0.7285 - val_acc: 0.8966\n",
      "\n",
      " Start of Epoch Learning Rate = 0.091058\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.7730 - acc: 0.8835\n",
      "\n",
      " End of Epoch Learning Rate = 0.090757\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7944 - acc: 0.8692 - val_loss: 0.7730 - val_acc: 0.8835\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090757\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 8s 763us/sample - loss: 0.7587 - acc: 0.8873\n",
      "\n",
      " End of Epoch Learning Rate = 0.090452\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7889 - acc: 0.8709 - val_loss: 0.7587 - val_acc: 0.8873\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090452\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.7889 - acc: 0.8808\n",
      "\n",
      " End of Epoch Learning Rate = 0.090142\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7808 - acc: 0.8747 - val_loss: 0.7889 - val_acc: 0.8808\n",
      "\n",
      " Start of Epoch Learning Rate = 0.090142\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 8s 762us/sample - loss: 0.7485 - acc: 0.8884\n",
      "\n",
      " End of Epoch Learning Rate = 0.089828\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7852 - acc: 0.8743 - val_loss: 0.7485 - val_acc: 0.8884\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089828\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.7754 - acc: 0.8839\n",
      "\n",
      " End of Epoch Learning Rate = 0.089509\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7876 - acc: 0.8723 - val_loss: 0.7754 - val_acc: 0.8839\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089509\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.7391 - acc: 0.8917\n",
      "\n",
      " End of Epoch Learning Rate = 0.089186\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7898 - acc: 0.8704 - val_loss: 0.7391 - val_acc: 0.8917\n",
      "\n",
      " Start of Epoch Learning Rate = 0.089186\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 8s 758us/sample - loss: 0.7652 - acc: 0.8845\n",
      "\n",
      " End of Epoch Learning Rate = 0.088858\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7905 - acc: 0.8702 - val_loss: 0.7652 - val_acc: 0.8845\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088858\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 0.7293 - acc: 0.8960\n",
      "\n",
      " End of Epoch Learning Rate = 0.088527\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7818 - acc: 0.8744 - val_loss: 0.7293 - val_acc: 0.8960\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088527\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.7637 - acc: 0.8827\n",
      "\n",
      " End of Epoch Learning Rate = 0.088191\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7793 - acc: 0.8744 - val_loss: 0.7637 - val_acc: 0.8827\n",
      "\n",
      " Start of Epoch Learning Rate = 0.088191\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.7408 - acc: 0.8893\n",
      "\n",
      " End of Epoch Learning Rate = 0.087851\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7674 - acc: 0.8766 - val_loss: 0.7408 - val_acc: 0.8893\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087851\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 8s 766us/sample - loss: 0.7593 - acc: 0.8929\n",
      "\n",
      " End of Epoch Learning Rate = 0.087507\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7698 - acc: 0.8753 - val_loss: 0.7593 - val_acc: 0.8929\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087507\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.7310 - acc: 0.8939\n",
      "\n",
      " End of Epoch Learning Rate = 0.087159\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7764 - acc: 0.8749 - val_loss: 0.7310 - val_acc: 0.8939\n",
      "\n",
      " Start of Epoch Learning Rate = 0.087159\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.7543 - acc: 0.8891\n",
      "\n",
      " End of Epoch Learning Rate = 0.086806\n",
      "400/400 [==============================] - 118s 295ms/step - loss: 0.7590 - acc: 0.8799 - val_loss: 0.7543 - val_acc: 0.8891\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086806\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.7578 - acc: 0.8844\n",
      "\n",
      " End of Epoch Learning Rate = 0.086450\n",
      "400/400 [==============================] - 117s 291ms/step - loss: 0.7688 - acc: 0.8769 - val_loss: 0.7578 - val_acc: 0.8844\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086450\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.7148 - acc: 0.8985\n",
      "\n",
      " End of Epoch Learning Rate = 0.086089\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7631 - acc: 0.8771 - val_loss: 0.7148 - val_acc: 0.8985\n",
      "\n",
      " Start of Epoch Learning Rate = 0.086089\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 0.7523 - acc: 0.8884\n",
      "\n",
      " End of Epoch Learning Rate = 0.085725\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7609 - acc: 0.8792 - val_loss: 0.7523 - val_acc: 0.8884\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085725\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 0.7352 - acc: 0.8936\n",
      "\n",
      " End of Epoch Learning Rate = 0.085357\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7572 - acc: 0.8777 - val_loss: 0.7352 - val_acc: 0.8936\n",
      "\n",
      " Start of Epoch Learning Rate = 0.085357\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.7176 - acc: 0.8964\n",
      "\n",
      " End of Epoch Learning Rate = 0.084985\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7530 - acc: 0.8784 - val_loss: 0.7176 - val_acc: 0.8964\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084985\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.7059 - acc: 0.8997\n",
      "\n",
      " End of Epoch Learning Rate = 0.084609\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7472 - acc: 0.8780 - val_loss: 0.7059 - val_acc: 0.8997\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084609\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.7094 - acc: 0.8952\n",
      "\n",
      " End of Epoch Learning Rate = 0.084229\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7514 - acc: 0.8778 - val_loss: 0.7094 - val_acc: 0.8952\n",
      "\n",
      " Start of Epoch Learning Rate = 0.084229\n",
      "Epoch 79/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.7117 - acc: 0.8977\n",
      "\n",
      " End of Epoch Learning Rate = 0.083845\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7449 - acc: 0.8806 - val_loss: 0.7117 - val_acc: 0.8977\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083845\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 7s 726us/sample - loss: 0.6807 - acc: 0.9026\n",
      "\n",
      " End of Epoch Learning Rate = 0.083458\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7513 - acc: 0.8776 - val_loss: 0.6807 - val_acc: 0.9026\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083458\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.6674 - acc: 0.9061\n",
      "\n",
      " End of Epoch Learning Rate = 0.083067\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7409 - acc: 0.8804 - val_loss: 0.6674 - val_acc: 0.9061\n",
      "\n",
      " Start of Epoch Learning Rate = 0.083067\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.7409 - acc: 0.8841\n",
      "\n",
      " End of Epoch Learning Rate = 0.082673\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.7358 - acc: 0.8816 - val_loss: 0.7409 - val_acc: 0.8841\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082673\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 0.6987 - acc: 0.8965\n",
      "\n",
      " End of Epoch Learning Rate = 0.082275\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7422 - acc: 0.8802 - val_loss: 0.6987 - val_acc: 0.8965\n",
      "\n",
      " Start of Epoch Learning Rate = 0.082275\n",
      "Epoch 84/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.7146 - acc: 0.8955\n",
      "\n",
      " End of Epoch Learning Rate = 0.081873\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7307 - acc: 0.8832 - val_loss: 0.7146 - val_acc: 0.8955\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081873\n",
      "Epoch 85/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 0.6805 - acc: 0.9037\n",
      "\n",
      " End of Epoch Learning Rate = 0.081468\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7384 - acc: 0.8803 - val_loss: 0.6805 - val_acc: 0.9037\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081468\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.6921 - acc: 0.9028\n",
      "\n",
      " End of Epoch Learning Rate = 0.081059\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7381 - acc: 0.8808 - val_loss: 0.6921 - val_acc: 0.9028\n",
      "\n",
      " Start of Epoch Learning Rate = 0.081059\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.7360 - acc: 0.8965\n",
      "\n",
      " End of Epoch Learning Rate = 0.080647\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7286 - acc: 0.8841 - val_loss: 0.7360 - val_acc: 0.8965\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080647\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 0.7653 - acc: 0.8803\n",
      "\n",
      " End of Epoch Learning Rate = 0.080232\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7268 - acc: 0.8839 - val_loss: 0.7653 - val_acc: 0.8803\n",
      "\n",
      " Start of Epoch Learning Rate = 0.080232\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 8s 751us/sample - loss: 0.7284 - acc: 0.8912\n",
      "\n",
      " End of Epoch Learning Rate = 0.079813\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7279 - acc: 0.8839 - val_loss: 0.7284 - val_acc: 0.8912\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079813\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.7296 - acc: 0.8945\n",
      "\n",
      " End of Epoch Learning Rate = 0.079391\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.7242 - acc: 0.8853 - val_loss: 0.7296 - val_acc: 0.8945\n",
      "\n",
      " Start of Epoch Learning Rate = 0.079391\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.7505 - acc: 0.8840\n",
      "\n",
      " End of Epoch Learning Rate = 0.078966\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7222 - acc: 0.8849 - val_loss: 0.7505 - val_acc: 0.8840\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078966\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.6694 - acc: 0.9047\n",
      "\n",
      " End of Epoch Learning Rate = 0.078538\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.7196 - acc: 0.8839 - val_loss: 0.6694 - val_acc: 0.9047\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078538\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 0.6630 - acc: 0.9061\n",
      "\n",
      " End of Epoch Learning Rate = 0.078106\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6993 - acc: 0.8898 - val_loss: 0.6630 - val_acc: 0.9061\n",
      "\n",
      " Start of Epoch Learning Rate = 0.078106\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.6966 - acc: 0.8987\n",
      "\n",
      " End of Epoch Learning Rate = 0.077672\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.7058 - acc: 0.8874 - val_loss: 0.6966 - val_acc: 0.8987\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077672\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.6909 - acc: 0.9044\n",
      "\n",
      " End of Epoch Learning Rate = 0.077234\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.7029 - acc: 0.8867 - val_loss: 0.6909 - val_acc: 0.9044\n",
      "\n",
      " Start of Epoch Learning Rate = 0.077234\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.6478 - acc: 0.9070\n",
      "\n",
      " End of Epoch Learning Rate = 0.076794\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6990 - acc: 0.8874 - val_loss: 0.6478 - val_acc: 0.9070\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076794\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.6591 - acc: 0.9073\n",
      "\n",
      " End of Epoch Learning Rate = 0.076350\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6867 - acc: 0.8904 - val_loss: 0.6591 - val_acc: 0.9073\n",
      "\n",
      " Start of Epoch Learning Rate = 0.076350\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.6686 - acc: 0.9033\n",
      "\n",
      " End of Epoch Learning Rate = 0.075904\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6924 - acc: 0.8883 - val_loss: 0.6686 - val_acc: 0.9033\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075904\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 0.6874 - acc: 0.8944\n",
      "\n",
      " End of Epoch Learning Rate = 0.075455\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6960 - acc: 0.8852 - val_loss: 0.6874 - val_acc: 0.8944\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075455\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.6670 - acc: 0.9013\n",
      "\n",
      " End of Epoch Learning Rate = 0.075002\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6972 - acc: 0.8874 - val_loss: 0.6670 - val_acc: 0.9013\n",
      "\n",
      " Start of Epoch Learning Rate = 0.075002\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.7286 - acc: 0.8865\n",
      "\n",
      " End of Epoch Learning Rate = 0.074548\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6890 - acc: 0.8902 - val_loss: 0.7286 - val_acc: 0.8865\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074548\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.6792 - acc: 0.9018\n",
      "\n",
      " End of Epoch Learning Rate = 0.074090\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6889 - acc: 0.8883 - val_loss: 0.6792 - val_acc: 0.9018\n",
      "\n",
      " Start of Epoch Learning Rate = 0.074090\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 7s 730us/sample - loss: 0.6811 - acc: 0.8977\n",
      "\n",
      " End of Epoch Learning Rate = 0.073630\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6808 - acc: 0.8901 - val_loss: 0.6811 - val_acc: 0.8977\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073630\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 731us/sample - loss: 0.6535 - acc: 0.9066\n",
      "\n",
      " End of Epoch Learning Rate = 0.073167\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6788 - acc: 0.8928 - val_loss: 0.6535 - val_acc: 0.9066\n",
      "\n",
      " Start of Epoch Learning Rate = 0.073167\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 0.6589 - acc: 0.9001\n",
      "\n",
      " End of Epoch Learning Rate = 0.072702\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6698 - acc: 0.8932 - val_loss: 0.6589 - val_acc: 0.9001\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072702\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.6436 - acc: 0.9040\n",
      "\n",
      " End of Epoch Learning Rate = 0.072235\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6637 - acc: 0.8935 - val_loss: 0.6436 - val_acc: 0.9040\n",
      "\n",
      " Start of Epoch Learning Rate = 0.072235\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.6673 - acc: 0.8974\n",
      "\n",
      " End of Epoch Learning Rate = 0.071764\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6657 - acc: 0.8930 - val_loss: 0.6673 - val_acc: 0.8974\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071764\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.6488 - acc: 0.9043\n",
      "\n",
      " End of Epoch Learning Rate = 0.071292\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6690 - acc: 0.8923 - val_loss: 0.6488 - val_acc: 0.9043\n",
      "\n",
      " Start of Epoch Learning Rate = 0.071292\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.6831 - acc: 0.8947\n",
      "\n",
      " End of Epoch Learning Rate = 0.070817\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6582 - acc: 0.8953 - val_loss: 0.6831 - val_acc: 0.8947\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070817\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.6607 - acc: 0.9079\n",
      "\n",
      " End of Epoch Learning Rate = 0.070340\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6633 - acc: 0.8932 - val_loss: 0.6607 - val_acc: 0.9079\n",
      "\n",
      " Start of Epoch Learning Rate = 0.070340\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.6326 - acc: 0.9089\n",
      "\n",
      " End of Epoch Learning Rate = 0.069860\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.6618 - acc: 0.8929 - val_loss: 0.6326 - val_acc: 0.9089\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069860\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.6300 - acc: 0.9105\n",
      "\n",
      " End of Epoch Learning Rate = 0.069379\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6459 - acc: 0.8967 - val_loss: 0.6300 - val_acc: 0.9105\n",
      "\n",
      " Start of Epoch Learning Rate = 0.069379\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.6268 - acc: 0.9088\n",
      "\n",
      " End of Epoch Learning Rate = 0.068895\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6425 - acc: 0.8970 - val_loss: 0.6268 - val_acc: 0.9088\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068895\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.6562 - acc: 0.9019\n",
      "\n",
      " End of Epoch Learning Rate = 0.068409\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6412 - acc: 0.8960 - val_loss: 0.6562 - val_acc: 0.9019\n",
      "\n",
      " Start of Epoch Learning Rate = 0.068409\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.6288 - acc: 0.9058\n",
      "\n",
      " End of Epoch Learning Rate = 0.067922\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6374 - acc: 0.8970 - val_loss: 0.6288 - val_acc: 0.9058\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067922\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.6247 - acc: 0.9076\n",
      "\n",
      " End of Epoch Learning Rate = 0.067432\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6367 - acc: 0.8970 - val_loss: 0.6247 - val_acc: 0.9076\n",
      "\n",
      " Start of Epoch Learning Rate = 0.067432\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.6378 - acc: 0.9031\n",
      "\n",
      " End of Epoch Learning Rate = 0.066940\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6238 - acc: 0.9022 - val_loss: 0.6378 - val_acc: 0.9031\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066940\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.6423 - acc: 0.9055\n",
      "\n",
      " End of Epoch Learning Rate = 0.066447\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6330 - acc: 0.8966 - val_loss: 0.6423 - val_acc: 0.9055\n",
      "\n",
      " Start of Epoch Learning Rate = 0.066447\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.6227 - acc: 0.9102\n",
      "\n",
      " End of Epoch Learning Rate = 0.065951\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6269 - acc: 0.8966 - val_loss: 0.6227 - val_acc: 0.9102\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065951\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.6027 - acc: 0.9112\n",
      "\n",
      " End of Epoch Learning Rate = 0.065454\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6284 - acc: 0.8978 - val_loss: 0.6027 - val_acc: 0.9112\n",
      "\n",
      " Start of Epoch Learning Rate = 0.065454\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 8s 769us/sample - loss: 0.6439 - acc: 0.9021\n",
      "\n",
      " End of Epoch Learning Rate = 0.064956\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6213 - acc: 0.9002 - val_loss: 0.6439 - val_acc: 0.9021\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064956\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.5742 - acc: 0.9219\n",
      "\n",
      " End of Epoch Learning Rate = 0.064455\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.6195 - acc: 0.8998 - val_loss: 0.5742 - val_acc: 0.9219\n",
      "\n",
      " Start of Epoch Learning Rate = 0.064455\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.5936 - acc: 0.9167\n",
      "\n",
      " End of Epoch Learning Rate = 0.063953\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.6081 - acc: 0.9030 - val_loss: 0.5936 - val_acc: 0.9167\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063953\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.6019 - acc: 0.9085\n",
      "\n",
      " End of Epoch Learning Rate = 0.063450\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6051 - acc: 0.9034 - val_loss: 0.6019 - val_acc: 0.9085\n",
      "\n",
      " Start of Epoch Learning Rate = 0.063450\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.5804 - acc: 0.9152\n",
      "\n",
      " End of Epoch Learning Rate = 0.062945\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.6011 - acc: 0.9035 - val_loss: 0.5804 - val_acc: 0.9152\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062945\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.5759 - acc: 0.9176\n",
      "\n",
      " End of Epoch Learning Rate = 0.062438\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.6053 - acc: 0.9029 - val_loss: 0.5759 - val_acc: 0.9176\n",
      "\n",
      " Start of Epoch Learning Rate = 0.062438\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.5839 - acc: 0.9123\n",
      "\n",
      " End of Epoch Learning Rate = 0.061930\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5971 - acc: 0.9031 - val_loss: 0.5839 - val_acc: 0.9123\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061930\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.5911 - acc: 0.9072\n",
      "\n",
      " End of Epoch Learning Rate = 0.061421\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5894 - acc: 0.9039 - val_loss: 0.5911 - val_acc: 0.9072\n",
      "\n",
      " Start of Epoch Learning Rate = 0.061421\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.6715 - acc: 0.8921\n",
      "\n",
      " End of Epoch Learning Rate = 0.060911\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5837 - acc: 0.9061 - val_loss: 0.6715 - val_acc: 0.8921\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060911\n",
      "Epoch 130/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.6020 - acc: 0.9086\n",
      "\n",
      " End of Epoch Learning Rate = 0.060400\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5820 - acc: 0.9068 - val_loss: 0.6020 - val_acc: 0.9086\n",
      "\n",
      " Start of Epoch Learning Rate = 0.060400\n",
      "Epoch 131/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.5845 - acc: 0.9137\n",
      "\n",
      " End of Epoch Learning Rate = 0.059887\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5883 - acc: 0.9037 - val_loss: 0.5845 - val_acc: 0.9137\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059887\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.5926 - acc: 0.9125\n",
      "\n",
      " End of Epoch Learning Rate = 0.059373\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5864 - acc: 0.9042 - val_loss: 0.5926 - val_acc: 0.9125\n",
      "\n",
      " Start of Epoch Learning Rate = 0.059373\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.5543 - acc: 0.9200\n",
      "\n",
      " End of Epoch Learning Rate = 0.058858\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5724 - acc: 0.9072 - val_loss: 0.5543 - val_acc: 0.9200\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058858\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 0.5635 - acc: 0.9175\n",
      "\n",
      " End of Epoch Learning Rate = 0.058343\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.5784 - acc: 0.9051 - val_loss: 0.5635 - val_acc: 0.9175\n",
      "\n",
      " Start of Epoch Learning Rate = 0.058343\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.5895 - acc: 0.9098\n",
      "\n",
      " End of Epoch Learning Rate = 0.057826\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5731 - acc: 0.9068 - val_loss: 0.5895 - val_acc: 0.9098\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057826\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.6413 - acc: 0.8932\n",
      "\n",
      " End of Epoch Learning Rate = 0.057308\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5647 - acc: 0.9088 - val_loss: 0.6413 - val_acc: 0.8932\n",
      "\n",
      " Start of Epoch Learning Rate = 0.057308\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.6069 - acc: 0.9086\n",
      "\n",
      " End of Epoch Learning Rate = 0.056790\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5569 - acc: 0.9104 - val_loss: 0.6069 - val_acc: 0.9086\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056790\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.5467 - acc: 0.9183\n",
      "\n",
      " End of Epoch Learning Rate = 0.056271\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5535 - acc: 0.9120 - val_loss: 0.5467 - val_acc: 0.9183\n",
      "\n",
      " Start of Epoch Learning Rate = 0.056271\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 0.5757 - acc: 0.9059\n",
      "\n",
      " End of Epoch Learning Rate = 0.055751\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5544 - acc: 0.9097 - val_loss: 0.5757 - val_acc: 0.9059\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055751\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.5496 - acc: 0.9178\n",
      "\n",
      " End of Epoch Learning Rate = 0.055231\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5487 - acc: 0.9106 - val_loss: 0.5496 - val_acc: 0.9178\n",
      "\n",
      " Start of Epoch Learning Rate = 0.055231\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.5351 - acc: 0.9185\n",
      "\n",
      " End of Epoch Learning Rate = 0.054710\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5478 - acc: 0.9082 - val_loss: 0.5351 - val_acc: 0.9185\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054710\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.5401 - acc: 0.9151\n",
      "\n",
      " End of Epoch Learning Rate = 0.054188\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5401 - acc: 0.9100 - val_loss: 0.5401 - val_acc: 0.9151\n",
      "\n",
      " Start of Epoch Learning Rate = 0.054188\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 0.5856 - acc: 0.9065\n",
      "\n",
      " End of Epoch Learning Rate = 0.053667\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5327 - acc: 0.9135 - val_loss: 0.5856 - val_acc: 0.9065\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053667\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.5300 - acc: 0.9195\n",
      "\n",
      " End of Epoch Learning Rate = 0.053144\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5289 - acc: 0.9141 - val_loss: 0.5300 - val_acc: 0.9195\n",
      "\n",
      " Start of Epoch Learning Rate = 0.053144\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.5512 - acc: 0.9140\n",
      "\n",
      " End of Epoch Learning Rate = 0.052622\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.5341 - acc: 0.9111 - val_loss: 0.5512 - val_acc: 0.9140\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052622\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.5227 - acc: 0.9225\n",
      "\n",
      " End of Epoch Learning Rate = 0.052099\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.5276 - acc: 0.9139 - val_loss: 0.5227 - val_acc: 0.9225\n",
      "\n",
      " Start of Epoch Learning Rate = 0.052099\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.5528 - acc: 0.9146\n",
      "\n",
      " End of Epoch Learning Rate = 0.051575\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5192 - acc: 0.9165 - val_loss: 0.5528 - val_acc: 0.9146\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051575\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 8s 751us/sample - loss: 0.5339 - acc: 0.9197\n",
      "\n",
      " End of Epoch Learning Rate = 0.051052\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.5193 - acc: 0.9148 - val_loss: 0.5339 - val_acc: 0.9197\n",
      "\n",
      " Start of Epoch Learning Rate = 0.051052\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 7s 735us/sample - loss: 0.5670 - acc: 0.9101\n",
      "\n",
      " End of Epoch Learning Rate = 0.050529\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5050 - acc: 0.9182 - val_loss: 0.5670 - val_acc: 0.9101\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050529\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.5279 - acc: 0.9144\n",
      "\n",
      " End of Epoch Learning Rate = 0.050005\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.5092 - acc: 0.9157 - val_loss: 0.5279 - val_acc: 0.9144\n",
      "\n",
      " Start of Epoch Learning Rate = 0.050005\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.5258 - acc: 0.9177\n",
      "\n",
      " End of Epoch Learning Rate = 0.049481\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.5010 - acc: 0.9168 - val_loss: 0.5258 - val_acc: 0.9177\n",
      "\n",
      " Start of Epoch Learning Rate = 0.049481\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.5864 - acc: 0.9030\n",
      "\n",
      " End of Epoch Learning Rate = 0.048958\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4947 - acc: 0.9194 - val_loss: 0.5864 - val_acc: 0.9030\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048958\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.5127 - acc: 0.9204\n",
      "\n",
      " End of Epoch Learning Rate = 0.048435\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4972 - acc: 0.9178 - val_loss: 0.5127 - val_acc: 0.9204\n",
      "\n",
      " Start of Epoch Learning Rate = 0.048435\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.5745 - acc: 0.9046\n",
      "\n",
      " End of Epoch Learning Rate = 0.047911\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4946 - acc: 0.9192 - val_loss: 0.5745 - val_acc: 0.9046\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047911\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 8s 755us/sample - loss: 0.5477 - acc: 0.9090\n",
      "\n",
      " End of Epoch Learning Rate = 0.047388\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4902 - acc: 0.9181 - val_loss: 0.5477 - val_acc: 0.9090\n",
      "\n",
      " Start of Epoch Learning Rate = 0.047388\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.4960 - acc: 0.9243\n",
      "\n",
      " End of Epoch Learning Rate = 0.046866\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4876 - acc: 0.9189 - val_loss: 0.4960 - val_acc: 0.9243\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046866\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 0.5497 - acc: 0.9078\n",
      "\n",
      " End of Epoch Learning Rate = 0.046343\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4742 - acc: 0.9222 - val_loss: 0.5497 - val_acc: 0.9078\n",
      "\n",
      " Start of Epoch Learning Rate = 0.046343\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 7s 731us/sample - loss: 0.5321 - acc: 0.9169\n",
      "\n",
      " End of Epoch Learning Rate = 0.045822\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4778 - acc: 0.9205 - val_loss: 0.5321 - val_acc: 0.9169\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045822\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 7s 728us/sample - loss: 0.4877 - acc: 0.9258\n",
      "\n",
      " End of Epoch Learning Rate = 0.045300\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.4688 - acc: 0.9225 - val_loss: 0.4877 - val_acc: 0.9258\n",
      "\n",
      " Start of Epoch Learning Rate = 0.045300\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.4750 - acc: 0.9267\n",
      "\n",
      " End of Epoch Learning Rate = 0.044779\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4632 - acc: 0.9239 - val_loss: 0.4750 - val_acc: 0.9267\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044779\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 8s 757us/sample - loss: 0.5157 - acc: 0.9150\n",
      "\n",
      " End of Epoch Learning Rate = 0.044259\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4673 - acc: 0.9230 - val_loss: 0.5157 - val_acc: 0.9150\n",
      "\n",
      " Start of Epoch Learning Rate = 0.044259\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 0.5022 - acc: 0.9207\n",
      "\n",
      " End of Epoch Learning Rate = 0.043739\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4545 - acc: 0.9255 - val_loss: 0.5022 - val_acc: 0.9207\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043739\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.4841 - acc: 0.9235\n",
      "\n",
      " End of Epoch Learning Rate = 0.043220\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4504 - acc: 0.9244 - val_loss: 0.4841 - val_acc: 0.9235\n",
      "\n",
      " Start of Epoch Learning Rate = 0.043220\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.5110 - acc: 0.9216\n",
      "\n",
      " End of Epoch Learning Rate = 0.042702\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.4469 - acc: 0.9260 - val_loss: 0.5110 - val_acc: 0.9216\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042702\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.5100 - acc: 0.9183\n",
      "\n",
      " End of Epoch Learning Rate = 0.042184\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4436 - acc: 0.9269 - val_loss: 0.5100 - val_acc: 0.9183\n",
      "\n",
      " Start of Epoch Learning Rate = 0.042184\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.5288 - acc: 0.9149\n",
      "\n",
      " End of Epoch Learning Rate = 0.041667\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4364 - acc: 0.9277 - val_loss: 0.5288 - val_acc: 0.9149\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041667\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.4668 - acc: 0.9316\n",
      "\n",
      " End of Epoch Learning Rate = 0.041152\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4342 - acc: 0.9265 - val_loss: 0.4668 - val_acc: 0.9316\n",
      "\n",
      " Start of Epoch Learning Rate = 0.041152\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.4745 - acc: 0.9245\n",
      "\n",
      " End of Epoch Learning Rate = 0.040637\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4335 - acc: 0.9277 - val_loss: 0.4745 - val_acc: 0.9245\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040637\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.4679 - acc: 0.9233\n",
      "\n",
      " End of Epoch Learning Rate = 0.040123\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4222 - acc: 0.9301 - val_loss: 0.4679 - val_acc: 0.9233\n",
      "\n",
      " Start of Epoch Learning Rate = 0.040123\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.4833 - acc: 0.9230\n",
      "\n",
      " End of Epoch Learning Rate = 0.039610\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4245 - acc: 0.9289 - val_loss: 0.4833 - val_acc: 0.9230\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039610\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.4390 - acc: 0.9280\n",
      "\n",
      " End of Epoch Learning Rate = 0.039099\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4175 - acc: 0.9305 - val_loss: 0.4390 - val_acc: 0.9280\n",
      "\n",
      " Start of Epoch Learning Rate = 0.039099\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.4618 - acc: 0.9251\n",
      "\n",
      " End of Epoch Learning Rate = 0.038589\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.4040 - acc: 0.9339 - val_loss: 0.4618 - val_acc: 0.9251\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038589\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.5026 - acc: 0.9144\n",
      "\n",
      " End of Epoch Learning Rate = 0.038080\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4078 - acc: 0.9319 - val_loss: 0.5026 - val_acc: 0.9144\n",
      "\n",
      " Start of Epoch Learning Rate = 0.038080\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 0.4823 - acc: 0.9214\n",
      "\n",
      " End of Epoch Learning Rate = 0.037572\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.4027 - acc: 0.9329 - val_loss: 0.4823 - val_acc: 0.9214\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037572\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.4718 - acc: 0.9221\n",
      "\n",
      " End of Epoch Learning Rate = 0.037065\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3967 - acc: 0.9329 - val_loss: 0.4718 - val_acc: 0.9221\n",
      "\n",
      " Start of Epoch Learning Rate = 0.037065\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 7s 727us/sample - loss: 0.4355 - acc: 0.9314\n",
      "\n",
      " End of Epoch Learning Rate = 0.036560\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3924 - acc: 0.9349 - val_loss: 0.4355 - val_acc: 0.9314\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036560\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.4400 - acc: 0.9295\n",
      "\n",
      " End of Epoch Learning Rate = 0.036057\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.3883 - acc: 0.9351 - val_loss: 0.4400 - val_acc: 0.9295\n",
      "\n",
      " Start of Epoch Learning Rate = 0.036057\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.4526 - acc: 0.9294\n",
      "\n",
      " End of Epoch Learning Rate = 0.035555\n",
      "400/400 [==============================] - 118s 295ms/step - loss: 0.3851 - acc: 0.9353 - val_loss: 0.4526 - val_acc: 0.9294\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035555\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.4881 - acc: 0.9170\n",
      "\n",
      " End of Epoch Learning Rate = 0.035054\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3770 - acc: 0.9383 - val_loss: 0.4881 - val_acc: 0.9170\n",
      "\n",
      " Start of Epoch Learning Rate = 0.035054\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 0.4344 - acc: 0.9298\n",
      "\n",
      " End of Epoch Learning Rate = 0.034556\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3723 - acc: 0.9393 - val_loss: 0.4344 - val_acc: 0.9298\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034556\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.4383 - acc: 0.9280\n",
      "\n",
      " End of Epoch Learning Rate = 0.034059\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3724 - acc: 0.9357 - val_loss: 0.4383 - val_acc: 0.9280\n",
      "\n",
      " Start of Epoch Learning Rate = 0.034059\n",
      "Epoch 182/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.4409 - acc: 0.9280\n",
      "\n",
      " End of Epoch Learning Rate = 0.033563\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3639 - acc: 0.9394 - val_loss: 0.4409 - val_acc: 0.9280\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033563\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.4198 - acc: 0.9309\n",
      "\n",
      " End of Epoch Learning Rate = 0.033070\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.3590 - acc: 0.9394 - val_loss: 0.4198 - val_acc: 0.9309\n",
      "\n",
      " Start of Epoch Learning Rate = 0.033070\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 7s 732us/sample - loss: 0.4479 - acc: 0.9240\n",
      "\n",
      " End of Epoch Learning Rate = 0.032578\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3635 - acc: 0.9384 - val_loss: 0.4479 - val_acc: 0.9240\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032578\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.4364 - acc: 0.9247\n",
      "\n",
      " End of Epoch Learning Rate = 0.032088\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3500 - acc: 0.9422 - val_loss: 0.4364 - val_acc: 0.9247\n",
      "\n",
      " Start of Epoch Learning Rate = 0.032088\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 8s 758us/sample - loss: 0.4731 - acc: 0.9126\n",
      "\n",
      " End of Epoch Learning Rate = 0.031601\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3539 - acc: 0.9404 - val_loss: 0.4731 - val_acc: 0.9126\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031601\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.4094 - acc: 0.9308\n",
      "\n",
      " End of Epoch Learning Rate = 0.031115\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3407 - acc: 0.9434 - val_loss: 0.4094 - val_acc: 0.9308\n",
      "\n",
      " Start of Epoch Learning Rate = 0.031115\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.4293 - acc: 0.9282\n",
      "\n",
      " End of Epoch Learning Rate = 0.030631\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3465 - acc: 0.9396 - val_loss: 0.4293 - val_acc: 0.9282\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030631\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 8s 774us/sample - loss: 0.4204 - acc: 0.9279\n",
      "\n",
      " End of Epoch Learning Rate = 0.030150\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3404 - acc: 0.9431 - val_loss: 0.4204 - val_acc: 0.9279\n",
      "\n",
      " Start of Epoch Learning Rate = 0.030150\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.4140 - acc: 0.9311\n",
      "\n",
      " End of Epoch Learning Rate = 0.029670\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3317 - acc: 0.9442 - val_loss: 0.4140 - val_acc: 0.9311\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029670\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.4226 - acc: 0.9255\n",
      "\n",
      " End of Epoch Learning Rate = 0.029193\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3310 - acc: 0.9437 - val_loss: 0.4226 - val_acc: 0.9255\n",
      "\n",
      " Start of Epoch Learning Rate = 0.029193\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.4050 - acc: 0.9331\n",
      "\n",
      " End of Epoch Learning Rate = 0.028718\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3276 - acc: 0.9442 - val_loss: 0.4050 - val_acc: 0.9331\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028718\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 0.4073 - acc: 0.9339\n",
      "\n",
      " End of Epoch Learning Rate = 0.028246\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3175 - acc: 0.9472 - val_loss: 0.4073 - val_acc: 0.9339\n",
      "\n",
      " Start of Epoch Learning Rate = 0.028246\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 0.4015 - acc: 0.9323\n",
      "\n",
      " End of Epoch Learning Rate = 0.027775\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3091 - acc: 0.9486 - val_loss: 0.4015 - val_acc: 0.9323\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027775\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.3918 - acc: 0.9327\n",
      "\n",
      " End of Epoch Learning Rate = 0.027308\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.3110 - acc: 0.9466 - val_loss: 0.3918 - val_acc: 0.9327\n",
      "\n",
      " Start of Epoch Learning Rate = 0.027308\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.4121 - acc: 0.9272\n",
      "\n",
      " End of Epoch Learning Rate = 0.026843\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3087 - acc: 0.9458 - val_loss: 0.4121 - val_acc: 0.9272\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026843\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.4198 - acc: 0.9242\n",
      "\n",
      " End of Epoch Learning Rate = 0.026380\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3022 - acc: 0.9481 - val_loss: 0.4198 - val_acc: 0.9242\n",
      "\n",
      " Start of Epoch Learning Rate = 0.026380\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.3965 - acc: 0.9302\n",
      "\n",
      " End of Epoch Learning Rate = 0.025920\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.3008 - acc: 0.9478 - val_loss: 0.3965 - val_acc: 0.9302\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025920\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 7s 729us/sample - loss: 0.4087 - acc: 0.9301\n",
      "\n",
      " End of Epoch Learning Rate = 0.025462\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2918 - acc: 0.9510 - val_loss: 0.4087 - val_acc: 0.9301\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025462\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.3740 - acc: 0.9325\n",
      "\n",
      " End of Epoch Learning Rate = 0.025007\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2905 - acc: 0.9499 - val_loss: 0.3740 - val_acc: 0.9325\n",
      "\n",
      " Start of Epoch Learning Rate = 0.025007\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.3807 - acc: 0.9349\n",
      "\n",
      " End of Epoch Learning Rate = 0.024555\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2879 - acc: 0.9506 - val_loss: 0.3807 - val_acc: 0.9349\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024555\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.4123 - acc: 0.9265\n",
      "\n",
      " End of Epoch Learning Rate = 0.024106\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2780 - acc: 0.9527 - val_loss: 0.4123 - val_acc: 0.9265\n",
      "\n",
      " Start of Epoch Learning Rate = 0.024106\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 7s 734us/sample - loss: 0.3869 - acc: 0.9295\n",
      "\n",
      " End of Epoch Learning Rate = 0.023660\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2770 - acc: 0.9528 - val_loss: 0.3869 - val_acc: 0.9295\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023660\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 7s 733us/sample - loss: 0.3980 - acc: 0.9271\n",
      "\n",
      " End of Epoch Learning Rate = 0.023216\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2708 - acc: 0.9535 - val_loss: 0.3980 - val_acc: 0.9271\n",
      "\n",
      " Start of Epoch Learning Rate = 0.023216\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.4001 - acc: 0.9327\n",
      "\n",
      " End of Epoch Learning Rate = 0.022776\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2608 - acc: 0.9566 - val_loss: 0.4001 - val_acc: 0.9327\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022776\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.3608 - acc: 0.9355\n",
      "\n",
      " End of Epoch Learning Rate = 0.022338\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2667 - acc: 0.9541 - val_loss: 0.3608 - val_acc: 0.9355\n",
      "\n",
      " Start of Epoch Learning Rate = 0.022338\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.3470 - acc: 0.9414\n",
      "\n",
      " End of Epoch Learning Rate = 0.021904\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2623 - acc: 0.9552 - val_loss: 0.3470 - val_acc: 0.9414\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021904\n",
      "Epoch 208/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.3500 - acc: 0.9420\n",
      "\n",
      " End of Epoch Learning Rate = 0.021472\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2503 - acc: 0.9583 - val_loss: 0.3500 - val_acc: 0.9420\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021472\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.3445 - acc: 0.9434\n",
      "\n",
      " End of Epoch Learning Rate = 0.021044\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2476 - acc: 0.9578 - val_loss: 0.3445 - val_acc: 0.9434\n",
      "\n",
      " Start of Epoch Learning Rate = 0.021044\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.3772 - acc: 0.9323\n",
      "\n",
      " End of Epoch Learning Rate = 0.020619\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.2472 - acc: 0.9573 - val_loss: 0.3772 - val_acc: 0.9323\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020619\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.3883 - acc: 0.9304\n",
      "\n",
      " End of Epoch Learning Rate = 0.020197\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2380 - acc: 0.9605 - val_loss: 0.3883 - val_acc: 0.9304\n",
      "\n",
      " Start of Epoch Learning Rate = 0.020197\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.3446 - acc: 0.9411\n",
      "\n",
      " End of Epoch Learning Rate = 0.019778\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2390 - acc: 0.9588 - val_loss: 0.3446 - val_acc: 0.9411\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019778\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.3450 - acc: 0.9373\n",
      "\n",
      " End of Epoch Learning Rate = 0.019363\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2339 - acc: 0.9596 - val_loss: 0.3450 - val_acc: 0.9373\n",
      "\n",
      " Start of Epoch Learning Rate = 0.019363\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.3699 - acc: 0.9347\n",
      "\n",
      " End of Epoch Learning Rate = 0.018951\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2400 - acc: 0.9576 - val_loss: 0.3699 - val_acc: 0.9347\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018951\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.3532 - acc: 0.9364\n",
      "\n",
      " End of Epoch Learning Rate = 0.018542\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2308 - acc: 0.9598 - val_loss: 0.3532 - val_acc: 0.9364\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018542\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.3628 - acc: 0.9370\n",
      "\n",
      " End of Epoch Learning Rate = 0.018137\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2255 - acc: 0.9605 - val_loss: 0.3628 - val_acc: 0.9370\n",
      "\n",
      " Start of Epoch Learning Rate = 0.018137\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.3635 - acc: 0.9347\n",
      "\n",
      " End of Epoch Learning Rate = 0.017735\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2234 - acc: 0.9609 - val_loss: 0.3635 - val_acc: 0.9347\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017735\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 8s 761us/sample - loss: 0.3289 - acc: 0.9433\n",
      "\n",
      " End of Epoch Learning Rate = 0.017337\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.2172 - acc: 0.9621 - val_loss: 0.3289 - val_acc: 0.9433\n",
      "\n",
      " Start of Epoch Learning Rate = 0.017337\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.3315 - acc: 0.9419\n",
      "\n",
      " End of Epoch Learning Rate = 0.016943\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.2075 - acc: 0.9653 - val_loss: 0.3315 - val_acc: 0.9419\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016943\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.3276 - acc: 0.9435\n",
      "\n",
      " End of Epoch Learning Rate = 0.016552\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.2044 - acc: 0.9658 - val_loss: 0.3276 - val_acc: 0.9435\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016552\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.3108 - acc: 0.9456\n",
      "\n",
      " End of Epoch Learning Rate = 0.016165\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.2022 - acc: 0.9660 - val_loss: 0.3108 - val_acc: 0.9456\n",
      "\n",
      " Start of Epoch Learning Rate = 0.016165\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.3420 - acc: 0.9373\n",
      "\n",
      " End of Epoch Learning Rate = 0.015781\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1992 - acc: 0.9656 - val_loss: 0.3420 - val_acc: 0.9373\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015781\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.3359 - acc: 0.9418\n",
      "\n",
      " End of Epoch Learning Rate = 0.015401\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1960 - acc: 0.9663 - val_loss: 0.3359 - val_acc: 0.9418\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015401\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 8s 751us/sample - loss: 0.3421 - acc: 0.9403\n",
      "\n",
      " End of Epoch Learning Rate = 0.015025\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.1900 - acc: 0.9676 - val_loss: 0.3421 - val_acc: 0.9403\n",
      "\n",
      " Start of Epoch Learning Rate = 0.015025\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 8s 760us/sample - loss: 0.3067 - acc: 0.9438\n",
      "\n",
      " End of Epoch Learning Rate = 0.014653\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.1916 - acc: 0.9669 - val_loss: 0.3067 - val_acc: 0.9438\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014653\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.3197 - acc: 0.9439\n",
      "\n",
      " End of Epoch Learning Rate = 0.014285\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1828 - acc: 0.9695 - val_loss: 0.3197 - val_acc: 0.9439\n",
      "\n",
      " Start of Epoch Learning Rate = 0.014285\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 8s 758us/sample - loss: 0.3300 - acc: 0.9421\n",
      "\n",
      " End of Epoch Learning Rate = 0.013921\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1792 - acc: 0.9695 - val_loss: 0.3300 - val_acc: 0.9421\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013921\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.3412 - acc: 0.9408\n",
      "\n",
      " End of Epoch Learning Rate = 0.013560\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1776 - acc: 0.9697 - val_loss: 0.3412 - val_acc: 0.9408\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013560\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.3166 - acc: 0.9454\n",
      "\n",
      " End of Epoch Learning Rate = 0.013204\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1751 - acc: 0.9696 - val_loss: 0.3166 - val_acc: 0.9454\n",
      "\n",
      " Start of Epoch Learning Rate = 0.013204\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.3115 - acc: 0.9458\n",
      "\n",
      " End of Epoch Learning Rate = 0.012851\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1682 - acc: 0.9713 - val_loss: 0.3115 - val_acc: 0.9458\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012851\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.2987 - acc: 0.9465\n",
      "\n",
      " End of Epoch Learning Rate = 0.012503\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1647 - acc: 0.9727 - val_loss: 0.2987 - val_acc: 0.9465\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012503\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 8s 763us/sample - loss: 0.2828 - acc: 0.9456\n",
      "\n",
      " End of Epoch Learning Rate = 0.012159\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1635 - acc: 0.9726 - val_loss: 0.2828 - val_acc: 0.9456\n",
      "\n",
      " Start of Epoch Learning Rate = 0.012159\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.3139 - acc: 0.9431\n",
      "\n",
      " End of Epoch Learning Rate = 0.011819\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1558 - acc: 0.9746 - val_loss: 0.3139 - val_acc: 0.9431\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011819\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.2902 - acc: 0.9450\n",
      "\n",
      " End of Epoch Learning Rate = 0.011483\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.1529 - acc: 0.9750 - val_loss: 0.2902 - val_acc: 0.9450\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011483\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.3091 - acc: 0.9448\n",
      "\n",
      " End of Epoch Learning Rate = 0.011152\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1512 - acc: 0.9753 - val_loss: 0.3091 - val_acc: 0.9448\n",
      "\n",
      " Start of Epoch Learning Rate = 0.011152\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.2875 - acc: 0.9481\n",
      "\n",
      " End of Epoch Learning Rate = 0.010824\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1450 - acc: 0.9761 - val_loss: 0.2875 - val_acc: 0.9481\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010824\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.3037 - acc: 0.9492\n",
      "\n",
      " End of Epoch Learning Rate = 0.010501\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.1424 - acc: 0.9760 - val_loss: 0.3037 - val_acc: 0.9492\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010501\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 8s 764us/sample - loss: 0.2940 - acc: 0.9476\n",
      "\n",
      " End of Epoch Learning Rate = 0.010182\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1428 - acc: 0.9763 - val_loss: 0.2940 - val_acc: 0.9476\n",
      "\n",
      " Start of Epoch Learning Rate = 0.010182\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.2986 - acc: 0.9455\n",
      "\n",
      " End of Epoch Learning Rate = 0.009868\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1413 - acc: 0.9755 - val_loss: 0.2986 - val_acc: 0.9455\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009868\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.2780 - acc: 0.9491\n",
      "\n",
      " End of Epoch Learning Rate = 0.009558\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1348 - acc: 0.9780 - val_loss: 0.2780 - val_acc: 0.9491\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009558\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.3054 - acc: 0.9484\n",
      "\n",
      " End of Epoch Learning Rate = 0.009253\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1336 - acc: 0.9782 - val_loss: 0.3054 - val_acc: 0.9484\n",
      "\n",
      " Start of Epoch Learning Rate = 0.009253\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.2849 - acc: 0.9496\n",
      "\n",
      " End of Epoch Learning Rate = 0.008952\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1308 - acc: 0.9784 - val_loss: 0.2849 - val_acc: 0.9496\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008952\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.2932 - acc: 0.9484\n",
      "\n",
      " End of Epoch Learning Rate = 0.008655\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.1292 - acc: 0.9791 - val_loss: 0.2932 - val_acc: 0.9484\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008655\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.3017 - acc: 0.9486\n",
      "\n",
      " End of Epoch Learning Rate = 0.008363\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1238 - acc: 0.9800 - val_loss: 0.3017 - val_acc: 0.9486\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008363\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.2887 - acc: 0.9481\n",
      "\n",
      " End of Epoch Learning Rate = 0.008076\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1218 - acc: 0.9801 - val_loss: 0.2887 - val_acc: 0.9481\n",
      "\n",
      " Start of Epoch Learning Rate = 0.008076\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.2761 - acc: 0.9536\n",
      "\n",
      " End of Epoch Learning Rate = 0.007793\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.1213 - acc: 0.9798 - val_loss: 0.2761 - val_acc: 0.9536\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007793\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.2819 - acc: 0.9499\n",
      "\n",
      " End of Epoch Learning Rate = 0.007515\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.1165 - acc: 0.9810 - val_loss: 0.2819 - val_acc: 0.9499\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007515\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.2775 - acc: 0.9508\n",
      "\n",
      " End of Epoch Learning Rate = 0.007241\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1147 - acc: 0.9811 - val_loss: 0.2775 - val_acc: 0.9508\n",
      "\n",
      " Start of Epoch Learning Rate = 0.007241\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.2865 - acc: 0.9520\n",
      "\n",
      " End of Epoch Learning Rate = 0.006972\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1128 - acc: 0.9817 - val_loss: 0.2865 - val_acc: 0.9520\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006972\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 8s 761us/sample - loss: 0.2512 - acc: 0.9552\n",
      "\n",
      " End of Epoch Learning Rate = 0.006708\n",
      "400/400 [==============================] - 118s 295ms/step - loss: 0.1102 - acc: 0.9823 - val_loss: 0.2512 - val_acc: 0.9552\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006708\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.2784 - acc: 0.9524\n",
      "\n",
      " End of Epoch Learning Rate = 0.006449\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.1098 - acc: 0.9818 - val_loss: 0.2784 - val_acc: 0.9524\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006449\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.2690 - acc: 0.9529\n",
      "\n",
      " End of Epoch Learning Rate = 0.006194\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.1034 - acc: 0.9842 - val_loss: 0.2690 - val_acc: 0.9529\n",
      "\n",
      " Start of Epoch Learning Rate = 0.006194\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 0.2752 - acc: 0.9511\n",
      "\n",
      " End of Epoch Learning Rate = 0.005944\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1020 - acc: 0.9840 - val_loss: 0.2752 - val_acc: 0.9511\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005944\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.2720 - acc: 0.9535\n",
      "\n",
      " End of Epoch Learning Rate = 0.005699\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.1000 - acc: 0.9849 - val_loss: 0.2720 - val_acc: 0.9535\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005699\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.2544 - acc: 0.9572\n",
      "\n",
      " End of Epoch Learning Rate = 0.005459\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0970 - acc: 0.9852 - val_loss: 0.2544 - val_acc: 0.9572\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005459\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.2628 - acc: 0.9566\n",
      "\n",
      " End of Epoch Learning Rate = 0.005224\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0972 - acc: 0.9850 - val_loss: 0.2628 - val_acc: 0.9566\n",
      "\n",
      " Start of Epoch Learning Rate = 0.005224\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.2671 - acc: 0.9543\n",
      "\n",
      " End of Epoch Learning Rate = 0.004994\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0945 - acc: 0.9858 - val_loss: 0.2671 - val_acc: 0.9543\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004994\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 8s 764us/sample - loss: 0.2579 - acc: 0.9546\n",
      "\n",
      " End of Epoch Learning Rate = 0.004768\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0933 - acc: 0.9861 - val_loss: 0.2579 - val_acc: 0.9546\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004768\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 8s 763us/sample - loss: 0.2796 - acc: 0.9521\n",
      "\n",
      " End of Epoch Learning Rate = 0.004548\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0911 - acc: 0.9861 - val_loss: 0.2796 - val_acc: 0.9521\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004548\n",
      "Epoch 260/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.2957 - acc: 0.9516\n",
      "\n",
      " End of Epoch Learning Rate = 0.004332\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.0883 - acc: 0.9869 - val_loss: 0.2957 - val_acc: 0.9516\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004332\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 8s 767us/sample - loss: 0.2482 - acc: 0.9558\n",
      "\n",
      " End of Epoch Learning Rate = 0.004122\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0877 - acc: 0.9872 - val_loss: 0.2482 - val_acc: 0.9558\n",
      "\n",
      " Start of Epoch Learning Rate = 0.004122\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.2565 - acc: 0.9577\n",
      "\n",
      " End of Epoch Learning Rate = 0.003916\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0863 - acc: 0.9871 - val_loss: 0.2565 - val_acc: 0.9577\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003916\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.2525 - acc: 0.9579\n",
      "\n",
      " End of Epoch Learning Rate = 0.003716\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0852 - acc: 0.9876 - val_loss: 0.2525 - val_acc: 0.9579\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003716\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.2743 - acc: 0.9531\n",
      "\n",
      " End of Epoch Learning Rate = 0.003521\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0815 - acc: 0.9890 - val_loss: 0.2743 - val_acc: 0.9531\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003521\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 8s 752us/sample - loss: 0.2593 - acc: 0.9557\n",
      "\n",
      " End of Epoch Learning Rate = 0.003331\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0817 - acc: 0.9883 - val_loss: 0.2593 - val_acc: 0.9557\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003331\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.2461 - acc: 0.9583\n",
      "\n",
      " End of Epoch Learning Rate = 0.003146\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0771 - acc: 0.9897 - val_loss: 0.2461 - val_acc: 0.9583\n",
      "\n",
      " Start of Epoch Learning Rate = 0.003146\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 8s 762us/sample - loss: 0.2400 - acc: 0.9601\n",
      "\n",
      " End of Epoch Learning Rate = 0.002966\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.0800 - acc: 0.9882 - val_loss: 0.2400 - val_acc: 0.9601\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002966\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.2506 - acc: 0.9560\n",
      "\n",
      " End of Epoch Learning Rate = 0.002791\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0774 - acc: 0.9895 - val_loss: 0.2506 - val_acc: 0.9560\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002791\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 8s 751us/sample - loss: 0.2472 - acc: 0.9601\n",
      "\n",
      " End of Epoch Learning Rate = 0.002621\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0730 - acc: 0.9905 - val_loss: 0.2472 - val_acc: 0.9601\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002621\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 8s 753us/sample - loss: 0.2697 - acc: 0.9566\n",
      "\n",
      " End of Epoch Learning Rate = 0.002457\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0754 - acc: 0.9900 - val_loss: 0.2697 - val_acc: 0.9566\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002457\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 7s 736us/sample - loss: 0.2531 - acc: 0.9577\n",
      "\n",
      " End of Epoch Learning Rate = 0.002298\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.0740 - acc: 0.9898 - val_loss: 0.2531 - val_acc: 0.9577\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002298\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 7s 739us/sample - loss: 0.2563 - acc: 0.9586\n",
      "\n",
      " End of Epoch Learning Rate = 0.002144\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0720 - acc: 0.9909 - val_loss: 0.2563 - val_acc: 0.9586\n",
      "\n",
      " Start of Epoch Learning Rate = 0.002144\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.2562 - acc: 0.9579\n",
      "\n",
      " End of Epoch Learning Rate = 0.001995\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0714 - acc: 0.9907 - val_loss: 0.2562 - val_acc: 0.9579\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001995\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 8s 759us/sample - loss: 0.2441 - acc: 0.9608\n",
      "\n",
      " End of Epoch Learning Rate = 0.001852\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0693 - acc: 0.9910 - val_loss: 0.2441 - val_acc: 0.9608\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001852\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.2400 - acc: 0.9619\n",
      "\n",
      " End of Epoch Learning Rate = 0.001714\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0688 - acc: 0.9915 - val_loss: 0.2400 - val_acc: 0.9619\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001714\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 0.2344 - acc: 0.9629\n",
      "\n",
      " End of Epoch Learning Rate = 0.001581\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0684 - acc: 0.9917 - val_loss: 0.2344 - val_acc: 0.9629\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001581\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 0.2500 - acc: 0.9605\n",
      "\n",
      " End of Epoch Learning Rate = 0.001453\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0684 - acc: 0.9913 - val_loss: 0.2500 - val_acc: 0.9605\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001453\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.2392 - acc: 0.9612\n",
      "\n",
      " End of Epoch Learning Rate = 0.001331\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0669 - acc: 0.9917 - val_loss: 0.2392 - val_acc: 0.9612\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001331\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 7s 750us/sample - loss: 0.2456 - acc: 0.9612\n",
      "\n",
      " End of Epoch Learning Rate = 0.001214\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0649 - acc: 0.9923 - val_loss: 0.2456 - val_acc: 0.9612\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001214\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 7s 738us/sample - loss: 0.2320 - acc: 0.9621\n",
      "\n",
      " End of Epoch Learning Rate = 0.001103\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0661 - acc: 0.9919 - val_loss: 0.2320 - val_acc: 0.9621\n",
      "\n",
      " Start of Epoch Learning Rate = 0.001103\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 7s 737us/sample - loss: 0.2345 - acc: 0.9633\n",
      "\n",
      " End of Epoch Learning Rate = 0.000996\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0655 - acc: 0.9922 - val_loss: 0.2345 - val_acc: 0.9633\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000996\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 8s 761us/sample - loss: 0.2398 - acc: 0.9616\n",
      "\n",
      " End of Epoch Learning Rate = 0.000896\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0635 - acc: 0.9929 - val_loss: 0.2398 - val_acc: 0.9616\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000896\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 8s 760us/sample - loss: 0.2334 - acc: 0.9627\n",
      "\n",
      " End of Epoch Learning Rate = 0.000800\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0645 - acc: 0.9924 - val_loss: 0.2334 - val_acc: 0.9627\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000800\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 7s 742us/sample - loss: 0.2264 - acc: 0.9630\n",
      "\n",
      " End of Epoch Learning Rate = 0.000710\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0638 - acc: 0.9929 - val_loss: 0.2264 - val_acc: 0.9630\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000710\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 7s 740us/sample - loss: 0.2340 - acc: 0.9627\n",
      "\n",
      " End of Epoch Learning Rate = 0.000626\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0619 - acc: 0.9932 - val_loss: 0.2340 - val_acc: 0.9627\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000626\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.2420 - acc: 0.9624\n",
      "\n",
      " End of Epoch Learning Rate = 0.000546\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0629 - acc: 0.9930 - val_loss: 0.2420 - val_acc: 0.9624\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000546\n",
      "Epoch 287/300\n",
      "10000/10000 [==============================] - 7s 741us/sample - loss: 0.2435 - acc: 0.9628\n",
      "\n",
      " End of Epoch Learning Rate = 0.000473\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0594 - acc: 0.9940 - val_loss: 0.2435 - val_acc: 0.9628\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000473\n",
      "Epoch 288/300\n",
      "10000/10000 [==============================] - 7s 746us/sample - loss: 0.2354 - acc: 0.9638\n",
      "\n",
      " End of Epoch Learning Rate = 0.000404\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0610 - acc: 0.9934 - val_loss: 0.2354 - val_acc: 0.9638\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000404\n",
      "Epoch 289/300\n",
      "10000/10000 [==============================] - 8s 765us/sample - loss: 0.2345 - acc: 0.9646\n",
      "\n",
      " End of Epoch Learning Rate = 0.000341\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.0610 - acc: 0.9932 - val_loss: 0.2345 - val_acc: 0.9646\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000341\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 0.2346 - acc: 0.9638\n",
      "\n",
      " End of Epoch Learning Rate = 0.000284\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0616 - acc: 0.9929 - val_loss: 0.2346 - val_acc: 0.9638\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000284\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 8s 773us/sample - loss: 0.2335 - acc: 0.9643\n",
      "\n",
      " End of Epoch Learning Rate = 0.000232\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0605 - acc: 0.9933 - val_loss: 0.2335 - val_acc: 0.9643\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000232\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 0.2341 - acc: 0.9651\n",
      "\n",
      " End of Epoch Learning Rate = 0.000185\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0596 - acc: 0.9937 - val_loss: 0.2341 - val_acc: 0.9651\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000185\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 8s 758us/sample - loss: 0.2330 - acc: 0.9644\n",
      "\n",
      " End of Epoch Learning Rate = 0.000144\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0591 - acc: 0.9937 - val_loss: 0.2330 - val_acc: 0.9644\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000144\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.2352 - acc: 0.9648\n",
      "\n",
      " End of Epoch Learning Rate = 0.000109\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.0598 - acc: 0.9938 - val_loss: 0.2352 - val_acc: 0.9648\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000109\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 7s 749us/sample - loss: 0.2344 - acc: 0.9652\n",
      "\n",
      " End of Epoch Learning Rate = 0.000079\n",
      "400/400 [==============================] - 117s 293ms/step - loss: 0.0605 - acc: 0.9935 - val_loss: 0.2344 - val_acc: 0.9652\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000079\n",
      "Epoch 296/300\n",
      "10000/10000 [==============================] - 7s 748us/sample - loss: 0.2357 - acc: 0.9650\n",
      "\n",
      " End of Epoch Learning Rate = 0.000054\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.0579 - acc: 0.9940 - val_loss: 0.2357 - val_acc: 0.9650\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000054\n",
      "Epoch 297/300\n",
      "10000/10000 [==============================] - 8s 763us/sample - loss: 0.2347 - acc: 0.9650\n",
      "\n",
      " End of Epoch Learning Rate = 0.000035\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0593 - acc: 0.9938 - val_loss: 0.2347 - val_acc: 0.9650\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000035\n",
      "Epoch 298/300\n",
      "10000/10000 [==============================] - 7s 744us/sample - loss: 0.2345 - acc: 0.9649\n",
      "\n",
      " End of Epoch Learning Rate = 0.000021\n",
      "400/400 [==============================] - 118s 294ms/step - loss: 0.0593 - acc: 0.9936 - val_loss: 0.2345 - val_acc: 0.9649\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000021\n",
      "Epoch 299/300\n",
      "10000/10000 [==============================] - 8s 763us/sample - loss: 0.2347 - acc: 0.9649\n",
      "\n",
      " End of Epoch Learning Rate = 0.000013\n",
      "400/400 [==============================] - 117s 294ms/step - loss: 0.0598 - acc: 0.9935 - val_loss: 0.2347 - val_acc: 0.9649\n",
      "\n",
      " Start of Epoch Learning Rate = 0.000013\n",
      "Epoch 300/300\n",
      "10000/10000 [==============================] - 7s 747us/sample - loss: 0.2352 - acc: 0.9652\n",
      "\n",
      " End of Epoch Learning Rate = 0.000010\n",
      "400/400 [==============================] - 117s 292ms/step - loss: 0.0608 - acc: 0.9935 - val_loss: 0.2352 - val_acc: 0.9652\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(dataGenerator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              steps_per_epoch =steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (%): 96.52\n"
     ]
    }
   ],
   "source": [
    "#get final performance\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print('Test accuracy (%):', 100*sum(np.argmax(y_pred,-1)==np.argmax(y_test,-1))/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss and accuracy\n",
    "plot_history(model.history.history)\n",
    "\n",
    "#plot learning rate schedule\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(np.arange(0,len(lr_scheduler.lr_used))/steps_per_epoch,lr_scheduler.lr_used)\n",
    "plt.xlabel('epoch number')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the weigts used for updating\n",
    "model.save_weights(ModelsPath+'Final_weights_'+WhichDataSet+'_OneBitPerWeight_model_sReLU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
